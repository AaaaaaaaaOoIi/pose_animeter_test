{"id":"static_image.js","dependencies":[{"name":"/Users/yamaguchiaoi/Desktop/pose_animeter/package.json","includedInParent":true,"mtime":1598233844173},{"name":"@tensorflow-models/posenet","loc":{"line":18,"column":32},"parent":"/Users/yamaguchiaoi/Desktop/pose_animeter/static_image.js","resolved":"/Users/yamaguchiaoi/Desktop/pose_animeter/node_modules/@tensorflow-models/posenet/dist/posenet.esm.js"},{"name":"@tensorflow-models/facemesh","loc":{"line":19,"column":33},"parent":"/Users/yamaguchiaoi/Desktop/pose_animeter/static_image.js","resolved":"/Users/yamaguchiaoi/Desktop/pose_animeter/node_modules/@tensorflow-models/facemesh/dist/facemesh.esm.js"},{"name":"@tensorflow/tfjs","loc":{"line":20,"column":20},"parent":"/Users/yamaguchiaoi/Desktop/pose_animeter/static_image.js","resolved":"/Users/yamaguchiaoi/Desktop/pose_animeter/node_modules/@tensorflow/tfjs/dist/tf.esm.js"},{"name":"paper","loc":{"line":21,"column":23},"parent":"/Users/yamaguchiaoi/Desktop/pose_animeter/static_image.js","resolved":"/Users/yamaguchiaoi/Desktop/pose_animeter/node_modules/paper/dist/paper-full.js"},{"name":"babel-polyfill","loc":{"line":22,"column":7},"parent":"/Users/yamaguchiaoi/Desktop/pose_animeter/static_image.js","resolved":"/Users/yamaguchiaoi/Desktop/pose_animeter/node_modules/babel-polyfill/lib/index.js"},{"name":"dat.gui","loc":{"line":24,"column":16},"parent":"/Users/yamaguchiaoi/Desktop/pose_animeter/static_image.js","resolved":"/Users/yamaguchiaoi/Desktop/pose_animeter/node_modules/dat.gui/build/dat.gui.module.js"},{"name":"./utils/svgUtils","loc":{"line":25,"column":23},"parent":"/Users/yamaguchiaoi/Desktop/pose_animeter/static_image.js","resolved":"/Users/yamaguchiaoi/Desktop/pose_animeter/utils/svgUtils.js"},{"name":"./illustrationGen/illustration","loc":{"line":26,"column":31},"parent":"/Users/yamaguchiaoi/Desktop/pose_animeter/static_image.js","resolved":"/Users/yamaguchiaoi/Desktop/pose_animeter/illustrationGen/illustration.js"},{"name":"./illustrationGen/skeleton","loc":{"line":27,"column":43},"parent":"/Users/yamaguchiaoi/Desktop/pose_animeter/static_image.js","resolved":"/Users/yamaguchiaoi/Desktop/pose_animeter/illustrationGen/skeleton.js"},{"name":"./utils/demoUtils","loc":{"line":48,"column":7},"parent":"/Users/yamaguchiaoi/Desktop/pose_animeter/static_image.js","resolved":"/Users/yamaguchiaoi/Desktop/pose_animeter/utils/demoUtils.js"},{"name":"./resources/illustration/boy.svg","loc":{"line":30,"column":24},"parent":"/Users/yamaguchiaoi/Desktop/pose_animeter/static_image.js","resolved":"/Users/yamaguchiaoi/Desktop/pose_animeter/resources/illustration/boy.svg"},{"name":"./resources/illustration/girl.svg","loc":{"line":31,"column":25},"parent":"/Users/yamaguchiaoi/Desktop/pose_animeter/static_image.js","resolved":"/Users/yamaguchiaoi/Desktop/pose_animeter/resources/illustration/girl.svg"},{"name":"./resources/illustration/abstract.svg","loc":{"line":32,"column":29},"parent":"/Users/yamaguchiaoi/Desktop/pose_animeter/static_image.js","resolved":"/Users/yamaguchiaoi/Desktop/pose_animeter/resources/illustration/abstract.svg"},{"name":"./resources/illustration/blathers.svg","loc":{"line":33,"column":29},"parent":"/Users/yamaguchiaoi/Desktop/pose_animeter/static_image.js","resolved":"/Users/yamaguchiaoi/Desktop/pose_animeter/resources/illustration/blathers.svg"},{"name":"./resources/illustration/tom-nook.svg","loc":{"line":34,"column":28},"parent":"/Users/yamaguchiaoi/Desktop/pose_animeter/static_image.js","resolved":"/Users/yamaguchiaoi/Desktop/pose_animeter/resources/illustration/tom-nook.svg"},{"name":"./resources/images/boy_doughnut.jpg","loc":{"line":35,"column":30},"parent":"/Users/yamaguchiaoi/Desktop/pose_animeter/static_image.js","resolved":"/Users/yamaguchiaoi/Desktop/pose_animeter/resources/images/boy_doughnut.jpg"},{"name":"./resources/images/tie_with_beer.jpg","loc":{"line":36,"column":31},"parent":"/Users/yamaguchiaoi/Desktop/pose_animeter/static_image.js","resolved":"/Users/yamaguchiaoi/Desktop/pose_animeter/resources/images/tie_with_beer.jpg"},{"name":"./resources/images/test.png","loc":{"line":37,"column":26},"parent":"/Users/yamaguchiaoi/Desktop/pose_animeter/static_image.js","resolved":"/Users/yamaguchiaoi/Desktop/pose_animeter/resources/images/test.png"},{"name":"./resources/images/full-body.png","loc":{"line":38,"column":27},"parent":"/Users/yamaguchiaoi/Desktop/pose_animeter/static_image.js","resolved":"/Users/yamaguchiaoi/Desktop/pose_animeter/resources/images/full-body.png"},{"name":"./resources/images/full-body_1.png","loc":{"line":39,"column":29},"parent":"/Users/yamaguchiaoi/Desktop/pose_animeter/static_image.js","resolved":"/Users/yamaguchiaoi/Desktop/pose_animeter/resources/images/full-body_1.png"},{"name":"./resources/images/full-body_2.png","loc":{"line":40,"column":29},"parent":"/Users/yamaguchiaoi/Desktop/pose_animeter/static_image.js","resolved":"/Users/yamaguchiaoi/Desktop/pose_animeter/resources/images/full-body_2.png"},{"name":"./utils/fileUtils","loc":{"line":49,"column":26},"parent":"/Users/yamaguchiaoi/Desktop/pose_animeter/static_image.js","resolved":"/Users/yamaguchiaoi/Desktop/pose_animeter/utils/fileUtils.js"}],"generated":{"js":"\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.bindPage = bindPage;\n\nvar posenet_module = _interopRequireWildcard(require(\"@tensorflow-models/posenet\"));\n\nvar facemesh_module = _interopRequireWildcard(require(\"@tensorflow-models/facemesh\"));\n\nvar tf = _interopRequireWildcard(require(\"@tensorflow/tfjs\"));\n\nvar paper = _interopRequireWildcard(require(\"paper\"));\n\nrequire(\"babel-polyfill\");\n\nvar _dat = _interopRequireDefault(require(\"dat.gui\"));\n\nvar _svgUtils = require(\"./utils/svgUtils\");\n\nvar _illustration = require(\"./illustrationGen/illustration\");\n\nvar _skeleton = require(\"./illustrationGen/skeleton\");\n\nvar _demoUtils = require(\"./utils/demoUtils\");\n\nvar boySVG = _interopRequireWildcard(require(\"./resources/illustration/boy.svg\"));\n\nvar girlSVG = _interopRequireWildcard(require(\"./resources/illustration/girl.svg\"));\n\nvar abstractSVG = _interopRequireWildcard(require(\"./resources/illustration/abstract.svg\"));\n\nvar blathersSVG = _interopRequireWildcard(require(\"./resources/illustration/blathers.svg\"));\n\nvar tomNookSVG = _interopRequireWildcard(require(\"./resources/illustration/tom-nook.svg\"));\n\nvar boy_doughnut = _interopRequireWildcard(require(\"./resources/images/boy_doughnut.jpg\"));\n\nvar tie_with_beer = _interopRequireWildcard(require(\"./resources/images/tie_with_beer.jpg\"));\n\nvar test_img = _interopRequireWildcard(require(\"./resources/images/test.png\"));\n\nvar full_body = _interopRequireWildcard(require(\"./resources/images/full-body.png\"));\n\nvar full_body_1 = _interopRequireWildcard(require(\"./resources/images/full-body_1.png\"));\n\nvar full_body_2 = _interopRequireWildcard(require(\"./resources/images/full-body_2.png\"));\n\nvar _fileUtils = require(\"./utils/fileUtils\");\n\nfunction _interopRequireDefault(obj) { return obj && obj.__esModule ? obj : { default: obj }; }\n\nfunction _getRequireWildcardCache() { if (typeof WeakMap !== \"function\") return null; var cache = new WeakMap(); _getRequireWildcardCache = function () { return cache; }; return cache; }\n\nfunction _interopRequireWildcard(obj) { if (obj && obj.__esModule) { return obj; } if (obj === null || typeof obj !== \"object\" && typeof obj !== \"function\") { return { default: obj }; } var cache = _getRequireWildcardCache(); if (cache && cache.has(obj)) { return cache.get(obj); } var newObj = {}; var hasPropertyDescriptor = Object.defineProperty && Object.getOwnPropertyDescriptor; for (var key in obj) { if (Object.prototype.hasOwnProperty.call(obj, key)) { var desc = hasPropertyDescriptor ? Object.getOwnPropertyDescriptor(obj, key) : null; if (desc && (desc.get || desc.set)) { Object.defineProperty(newObj, key, desc); } else { newObj[key] = obj[key]; } } } newObj.default = obj; if (cache) { cache.set(obj, newObj); } return newObj; }\n\nfunction asyncGeneratorStep(gen, resolve, reject, _next, _throw, key, arg) { try { var info = gen[key](arg); var value = info.value; } catch (error) { reject(error); return; } if (info.done) { resolve(value); } else { Promise.resolve(value).then(_next, _throw); } }\n\nfunction _asyncToGenerator(fn) { return function () { var self = this, args = arguments; return new Promise(function (resolve, reject) { var gen = fn.apply(self, args); function _next(value) { asyncGeneratorStep(gen, resolve, reject, _next, _throw, \"next\", value); } function _throw(err) { asyncGeneratorStep(gen, resolve, reject, _next, _throw, \"throw\", err); } _next(undefined); }); }; }\n\n// clang-format on\nvar resnetArchitectureName = 'MobileNetV1';\nvar avatarSvgs = {\n  'girl': girlSVG.default,\n  'boy': boySVG.default,\n  'abstract': abstractSVG.default,\n  'blathers': blathersSVG.default,\n  'tom-nook': tomNookSVG.default\n};\nvar sourceImages = {\n  'boy_doughnut': boy_doughnut.default,\n  'tie_with_beer': tie_with_beer.default,\n  'test_img': test_img.default,\n  'full_body': full_body.default,\n  'full_body_1': full_body_1.default,\n  'full_body_2': full_body_2.default\n};\nvar skeleton;\nvar illustration;\nvar canvasScope;\nvar posenet;\nvar facemesh;\nvar VIDEO_WIDTH = 513;\nvar VIDEO_HEIGHT = 513;\nvar CANVAS_WIDTH = 513;\nvar CANVAS_HEIGHT = 513;\nvar defaultQuantBytes = 2;\nvar defaultMultiplier = 1.0;\nvar defaultStride = 16;\nvar defaultInputResolution = 257;\nvar defaultMaxDetections = 1;\nvar defaultMinPartConfidence = 0.1;\nvar defaultMinPoseConfidence = 0.2;\nvar defaultNmsRadius = 20.0;\nvar predictedPoses;\nvar faceDetection;\nvar sourceImage;\n/**\n * Draws a pose if it passes a minimum confidence onto a canvas.\n * Only the pose's keypoints that pass a minPartConfidence are drawn.\n */\n\nfunction drawResults(image, canvas, faceDetection, poses) {\n  (0, _demoUtils.renderImageToCanvas)(image, [VIDEO_WIDTH, VIDEO_HEIGHT], canvas);\n  var ctx = canvas.getContext('2d');\n  poses.forEach(function (pose) {\n    if (pose.score >= defaultMinPoseConfidence) {\n      if (guiState.showKeypoints) {\n        (0, _demoUtils.drawKeypoints)(pose.keypoints, defaultMinPartConfidence, ctx);\n      }\n\n      if (guiState.showSkeleton) {\n        (0, _demoUtils.drawSkeleton)(pose.keypoints, defaultMinPartConfidence, ctx);\n      }\n    }\n  });\n\n  if (guiState.showKeypoints) {\n    faceDetection.forEach(function (face) {\n      Object.values(_skeleton.facePartName2Index).forEach(function (index) {\n        var p = face.scaledMesh[index];\n        (0, _demoUtils.drawPoint)(ctx, p[1], p[0], 3, 'red');\n      });\n    });\n  }\n}\n\nfunction loadImage(_x) {\n  return _loadImage.apply(this, arguments);\n}\n\nfunction _loadImage() {\n  _loadImage = _asyncToGenerator( /*#__PURE__*/regeneratorRuntime.mark(function _callee(imagePath) {\n    var image, promise;\n    return regeneratorRuntime.wrap(function _callee$(_context) {\n      while (1) {\n        switch (_context.prev = _context.next) {\n          case 0:\n            image = new Image();\n            promise = new Promise(function (resolve, reject) {\n              image.crossOrigin = '';\n\n              image.onload = function () {\n                resolve(image);\n              };\n            });\n            image.src = imagePath;\n            return _context.abrupt(\"return\", promise);\n\n          case 4:\n          case \"end\":\n            return _context.stop();\n        }\n      }\n    }, _callee);\n  }));\n  return _loadImage.apply(this, arguments);\n}\n\nfunction multiPersonCanvas() {\n  return document.querySelector('#multi canvas');\n}\n\nfunction getIllustrationCanvas() {\n  return document.querySelector('.illustration-canvas');\n}\n/**\n * Draw the results from the multi-pose estimation on to a canvas\n */\n\n\nfunction drawDetectionResults() {\n  var canvas = multiPersonCanvas();\n  drawResults(sourceImage, canvas, faceDetection, predictedPoses);\n\n  if (!predictedPoses || !predictedPoses.length || !illustration) {\n    return;\n  }\n\n  skeleton.reset();\n  canvasScope.project.clear();\n\n  if (faceDetection && faceDetection.length > 0) {\n    var face = _skeleton.Skeleton.toFaceFrame(faceDetection[0]);\n\n    illustration.updateSkeleton(predictedPoses[0], face);\n  } else {\n    illustration.updateSkeleton(predictedPoses[0], null);\n  }\n\n  illustration.draw(canvasScope, sourceImage.width, sourceImage.height);\n\n  if (guiState.showCurves) {\n    illustration.debugDraw(canvasScope);\n  }\n\n  if (guiState.showLabels) {\n    illustration.debugDrawLabel(canvasScope);\n  }\n}\n/**\n * Loads an image, feeds it into posenet the posenet model, and\n * calculates poses based on the model outputs\n */\n\n\nfunction testImageAndEstimatePoses() {\n  return _testImageAndEstimatePoses.apply(this, arguments);\n}\n\nfunction _testImageAndEstimatePoses() {\n  _testImageAndEstimatePoses = _asyncToGenerator( /*#__PURE__*/regeneratorRuntime.mark(function _callee2() {\n    return regeneratorRuntime.wrap(function _callee2$(_context2) {\n      while (1) {\n        switch (_context2.prev = _context2.next) {\n          case 0:\n            (0, _demoUtils.toggleLoadingUI)(true);\n            (0, _demoUtils.setStatusText)('Loading FaceMesh model...');\n            document.getElementById('results').style.display = 'none'; // Reload facemesh model to purge states from previous runs.\n\n            _context2.next = 5;\n            return facemesh_module.load();\n\n          case 5:\n            facemesh = _context2.sent;\n            // Load an example image\n            (0, _demoUtils.setStatusText)('Loading image...');\n            _context2.next = 9;\n            return loadImage(sourceImages[guiState.sourceImage]);\n\n          case 9:\n            sourceImage = _context2.sent;\n            // Estimates poses\n            (0, _demoUtils.setStatusText)('Predicting...');\n            _context2.next = 13;\n            return posenet.estimatePoses(sourceImage, {\n              flipHorizontal: false,\n              decodingMethod: 'multi-person',\n              maxDetections: defaultMaxDetections,\n              scoreThreshold: defaultMinPartConfidence,\n              nmsRadius: defaultNmsRadius\n            });\n\n          case 13:\n            predictedPoses = _context2.sent;\n            _context2.next = 16;\n            return facemesh.estimateFaces(sourceImage, false, false);\n\n          case 16:\n            faceDetection = _context2.sent;\n            // Draw poses.\n            drawDetectionResults();\n            (0, _demoUtils.toggleLoadingUI)(false);\n            document.getElementById('results').style.display = 'block';\n\n          case 20:\n          case \"end\":\n            return _context2.stop();\n        }\n      }\n    }, _callee2);\n  }));\n  return _testImageAndEstimatePoses.apply(this, arguments);\n}\n\nvar guiState = {\n  // Selected image\n  sourceImage: Object.keys(sourceImages)[0],\n  avatarSVG: Object.keys(avatarSvgs)[0],\n  // Detection debug\n  showKeypoints: true,\n  showSkeleton: true,\n  // Illustration debug\n  showCurves: false,\n  showLabels: false\n};\n\nfunction setupGui() {\n  var gui = new _dat.default.GUI();\n  var imageControls = gui.addFolder('Image');\n  imageControls.open();\n  gui.add(guiState, 'sourceImage', Object.keys(sourceImages)).onChange(function () {\n    return testImageAndEstimatePoses();\n  });\n  gui.add(guiState, 'avatarSVG', Object.keys(avatarSvgs)).onChange(function () {\n    return loadSVG(avatarSvgs[guiState.avatarSVG]);\n  });\n  var debugControls = gui.addFolder('Debug controls');\n  debugControls.open();\n  gui.add(guiState, 'showKeypoints').onChange(drawDetectionResults);\n  gui.add(guiState, 'showSkeleton').onChange(drawDetectionResults);\n  gui.add(guiState, 'showCurves').onChange(drawDetectionResults);\n  gui.add(guiState, 'showLabels').onChange(drawDetectionResults);\n}\n/**\n * Kicks off the demo by loading the posenet model and estimating\n * poses on a default image\n */\n\n\nfunction bindPage() {\n  return _bindPage.apply(this, arguments);\n}\n\nfunction _bindPage() {\n  _bindPage = _asyncToGenerator( /*#__PURE__*/regeneratorRuntime.mark(function _callee3() {\n    var canvas;\n    return regeneratorRuntime.wrap(function _callee3$(_context3) {\n      while (1) {\n        switch (_context3.prev = _context3.next) {\n          case 0:\n            (0, _demoUtils.toggleLoadingUI)(true);\n            canvasScope = paper.default;\n            canvas = getIllustrationCanvas();\n            canvas.width = CANVAS_WIDTH;\n            canvas.height = CANVAS_HEIGHT;\n            canvasScope.setup(canvas);\n            _context3.next = 8;\n            return tf.setBackend('webgl');\n\n          case 8:\n            (0, _demoUtils.setStatusText)('Loading PoseNet model...');\n            _context3.next = 11;\n            return posenet_module.load({\n              architecture: resnetArchitectureName,\n              outputStride: defaultStride,\n              inputResolution: defaultInputResolution,\n              multiplier: defaultMultiplier,\n              quantBytes: defaultQuantBytes\n            });\n\n          case 11:\n            posenet = _context3.sent;\n            setupGui(posenet);\n            (0, _demoUtils.setStatusText)('Loading SVG file...');\n            _context3.next = 16;\n            return loadSVG(Object.values(avatarSvgs)[0]);\n\n          case 16:\n          case \"end\":\n            return _context3.stop();\n        }\n      }\n    }, _callee3);\n  }));\n  return _bindPage.apply(this, arguments);\n}\n\nwindow.onload = bindPage;\n\n_fileUtils.FileUtils.setDragDropHandler(loadSVG); // Target is SVG string or path\n\n\nfunction loadSVG(_x2) {\n  return _loadSVG.apply(this, arguments);\n}\n\nfunction _loadSVG() {\n  _loadSVG = _asyncToGenerator( /*#__PURE__*/regeneratorRuntime.mark(function _callee4(target) {\n    var svgScope;\n    return regeneratorRuntime.wrap(function _callee4$(_context4) {\n      while (1) {\n        switch (_context4.prev = _context4.next) {\n          case 0:\n            _context4.next = 2;\n            return _svgUtils.SVGUtils.importSVG(target);\n\n          case 2:\n            svgScope = _context4.sent;\n            skeleton = new _skeleton.Skeleton(svgScope);\n            illustration = new _illustration.PoseIllustration(canvasScope);\n            illustration.bindSkeleton(skeleton, svgScope);\n            testImageAndEstimatePoses();\n\n          case 7:\n          case \"end\":\n            return _context4.stop();\n        }\n      }\n    }, _callee4);\n  }));\n  return _loadSVG.apply(this, arguments);\n}"},"sourceMaps":null,"error":null,"hash":"f304b35b035306dff0137727125a386b","cacheData":{"env":{}}}